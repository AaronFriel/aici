[package]
name = "rllm"
version = "0.1.0"
edition = "2021"
default-run = "rllm-server"

[dependencies]
anyhow = "1.0.75"
clap = "4.4.8"
hf-hub = "0.3.2"
tokenizers = { version = "0.15.0", features = ["hf-hub"] }
serde_json = "1.0.108"
serde = { version = "1.0.193", features = ["derive"] }
rand = "0.8.5"
half = "2.3.1"
log = "0.4.20"
actix-web = "4.4.0"
tokio = { version = "1.34.0", features = ["sync"] }
futures = "0.3.29"
uuid = { version = "1.6.1", features = ["v4"] }
derive_more = "0.99.17"

tch = { version = "0.14.0", optional = true }
torch-sys = { version = "0.14.0", optional = true }

cudarc = { version = "0.10.0", features = ["f16"], optional = true }
tch-cuda = { path = "../tch-cuda", optional = true }

llama_cpp_low = { path = "../llama-cpp-low", optional = true }

aicirt = { path = "../aicirt" }
aici_abi = { path = "../aici_abi" }
libc = "0.2.150"
base64 = "0.21.5"
indicatif = "0.17.7"
memmap2 = "0.9.0"
safetensors = "0.4.1"
lazy_static = "1.4.0"
fxhash = "0.2.1"
cfg-if = "1.0.0"
percent-encoding = "2.3.1"

[[bin]]
name = "rllm-server"
path = "src/driver.rs"

[build-dependencies]
anyhow = { version = "1", features = ["backtrace"] }
num_cpus = "1.15.0"
rayon = "1.7.0"

[features]
#default = ["llamacpp"]
default = ["tch", "cuda"]
tch = ["dep:tch", "dep:torch-sys"]
cuda = ["tch", "dep:tch-cuda", "dep:cudarc", "llama_cpp_low?/cuda"]
llamacpp = ["dep:llama_cpp_low"]
