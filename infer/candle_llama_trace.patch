diff --git a/candle-transformers/src/models/llama.rs b/candle-transformers/src/models/llama.rs
index 7e8c892..05da16d 100644
--- a/candle-transformers/src/models/llama.rs
+++ b/candle-transformers/src/models/llama.rs
@@ -205,6 +205,7 @@ impl CausalSelfAttention {
     }
 
     fn forward(&self, x: &Tensor, index_pos: usize, block_idx: usize) -> Result<Tensor> {
+        let log = false && block_idx <= 1;
         let _enter = self.span.enter();
         let (b_sz, seq_len, hidden_size) = x.dims3()?;
         let q = self.q_proj.forward(x)?;
@@ -224,6 +225,11 @@ impl CausalSelfAttention {
         let q = self.apply_rotary_emb(&q, index_pos)?;
         let mut k = self.apply_rotary_emb(&k, index_pos)?;
 
+        if log {
+            println!("q: {}", q);
+            println!("k: {}", k);
+        }
+
         if self.cache.use_kv_cache {
             let mut cache = self.cache.kvs.lock().unwrap();
             if let Some((cache_k, cache_v)) = &cache[block_idx] {
@@ -248,6 +254,11 @@ impl CausalSelfAttention {
         let k = self.repeat_kv(k)?;
         let v = self.repeat_kv(v)?;
 
+        if log {
+            println!("q2: {}", q);
+            println!("k2: {}", k);
+        }
+
         let y = if self.use_flash_attn {
             // flash-attn expects (b_sz, seq_len, nheads, head_dim)
             let q = q.transpose(1, 2)?;
@@ -267,6 +278,13 @@ impl CausalSelfAttention {
             // Convert to contiguous as matmul doesn't support strided vs for now.
             att.matmul(&v.contiguous()?)?.to_dtype(in_dtype)?
         };
+
+
+
+        if log {
+            println!("y: {}", y);
+        }
+
         let y = y.transpose(1, 2)?.reshape(&[b_sz, seq_len, hidden_size])?;
         let y = self.o_proj.forward(&y)?;
         Ok(y)
@@ -365,6 +383,7 @@ impl Block {
         let x = (self.attn.forward(&x, index_pos, block_idx)? + residual)?;
         let residual = &x;
         let x = (self.mlp.forward(&self.rms_2.forward(&x)?)? + residual)?;
+        // println!("x: {}", x);
         Ok(x)
     }
 
@@ -403,7 +422,9 @@ impl Llama {
             x = block.forward(&x, index_pos, block_idx)?;
         }
         let x = self.ln_f.forward(&x)?;
+        println!("x: {}", x);
         let x = x.i((.., seq_len - 1, ..))?;
+        println!("x1: {}", x);
         let logits = self.lm_head.forward(&x)?;
         logits.to_dtype(DType::F32)
     }
