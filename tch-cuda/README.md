# tch-cuda

based on
https://github.com/Dao-AILab/flash-attention/tree/9356a1c0389660d7e231ff3163c1ac17d9e3824a/csrc/flash_attn/src


